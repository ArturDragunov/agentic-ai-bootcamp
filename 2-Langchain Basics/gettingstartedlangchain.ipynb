{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ae95606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8f3c76b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Agentic2.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dd7e631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv(\"LANGCHAIN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a95342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\") # keep these exact names as they would be looked for in langchain\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "## Langsmith Tracking And Tracing\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\" # this is for langsmith tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47124a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x000001E26FCE1350> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001E26FCE3100> root_client=<openai.OpenAI object at 0x000001E26F9C2EA0> root_async_client=<openai.AsyncOpenAI object at 0x000001E26FCE10F0> model_name='gpt-4.1-nano-2025-04-14' model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm=ChatOpenAI(model=\"gpt-4.1-nano-2025-04-14\") # cost optimized model for real time applications\n",
    "# llm=ChatOpenAI(model=\"gpt-4.1-mini-2025-04-14\") # for more complex tasks\n",
    "print(llm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35ca146d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='**Agentic AI** refers to artificial intelligence systems that possess or exhibit *agency*‚Äîthat is, the ability to operate autonomously, make decisions, set goals, and take actions to achieve those goals in a dynamic environment without constant human intervention.\\n\\n### Key Characteristics of Agentic AI:\\n- **Autonomy:** It can perform tasks and make decisions independently.\\n- **Goal-oriented behavior:** It can establish or be assigned objectives and work towards achieving them.\\n- **Adaptability:** It can perceive changes in its environment and adjust its actions accordingly.\\n- **Proactiveness:** Instead of merely reacting to inputs, agentic AI anticipates needs or plans steps ahead.\\n\\n### Examples and Context:\\n- **Intelligent Agents:** Software programs that act on behalf of a user, such as virtual assistants (e.g., Siri, Alexa) that not only respond but can schedule meetings, send reminders, or reorder supplies based on set preferences.\\n- **Robotic Systems:** Robots navigating complex environments or performing tasks like delivery or maintenance autonomously.\\n- **AI in Gaming:** Non-player characters (NPCs) that adapt strategies dynamically.\\n\\n### Why it matters:\\nAgentic AI marks a shift from traditional AI systems that are reactive and follow explicit instructions toward systems that simulate some level of independent ‚Äúintentionality.‚Äù This increases AI‚Äôs utility but also brings considerations around control, ethics, and responsibility.\\n\\n---\\n\\nIf you want, I can also provide detailed examples or talk about theoretical implications of agentic AI. Let me know!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 301, 'prompt_tokens': 12, 'total_tokens': 313, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_79b79be41f', 'id': 'chatcmpl-Bb0mwRV6VtVVpXitg5YQhpygCgK2P', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--0637f7ef-f878-4087-9d64-fcfbb757bf47-0' usage_metadata={'input_tokens': 12, 'output_tokens': 301, 'total_tokens': 313, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "result=llm.invoke(\"What is Agentic AI\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41d6b3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Agentic AI** refers to artificial intelligence systems that possess or exhibit *agency*‚Äîthat is, the ability to operate autonomously, make decisions, set goals, and take actions to achieve those goals in a dynamic environment without constant human intervention.\n",
      "\n",
      "### Key Characteristics of Agentic AI:\n",
      "- **Autonomy:** It can perform tasks and make decisions independently.\n",
      "- **Goal-oriented behavior:** It can establish or be assigned objectives and work towards achieving them.\n",
      "- **Adaptability:** It can perceive changes in its environment and adjust its actions accordingly.\n",
      "- **Proactiveness:** Instead of merely reacting to inputs, agentic AI anticipates needs or plans steps ahead.\n",
      "\n",
      "### Examples and Context:\n",
      "- **Intelligent Agents:** Software programs that act on behalf of a user, such as virtual assistants (e.g., Siri, Alexa) that not only respond but can schedule meetings, send reminders, or reorder supplies based on set preferences.\n",
      "- **Robotic Systems:** Robots navigating complex environments or performing tasks like delivery or maintenance autonomously.\n",
      "- **AI in Gaming:** Non-player characters (NPCs) that adapt strategies dynamically.\n",
      "\n",
      "### Why it matters:\n",
      "Agentic AI marks a shift from traditional AI systems that are reactive and follow explicit instructions toward systems that simulate some level of independent ‚Äúintentionality.‚Äù This increases AI‚Äôs utility but also brings considerations around control, ethics, and responsibility.\n",
      "\n",
      "---\n",
      "\n",
      "If you want, I can also provide detailed examples or talk about theoretical implications of agentic AI. Let me know!\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8357dd50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n<think>\\nOkay, the user said \"Hi My name is Krish.\" I need to respond appropriately. Let me start by greeting them back. I should use their name to make it personal. Maybe say something like \"Hello Krish!\" to keep it friendly.\\n\\nNext, I should offer assistance. They might need help with something specific, so asking how I can assist them would be good. I should make sure to keep the tone welcoming and open. Let me check if there\\'s anything else I should consider. Since they introduced themselves, maybe they\\'re new here. I should encourage them to ask questions or share what they need help with. Alright, that should cover a helpful and friendly response without being too verbose.\\n</think>\\n\\nHello Krish! Nice to meet you. How can I assist you today? Feel free to ask me any questions or let me know if you need help with anything specific! üòä'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"qwen-qwq-32b\") # reasoning model - we see it chain of thought\n",
    "model.invoke(\"Hi My name is Krish\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1529904b",
   "metadata": {},
   "source": [
    " LangChain internally handles string formatting using the Jinja-like templating system. You‚Äôre not building the string yourself ‚Äî you're passing a template object with variable placeholders. LangChain will later inject the value of \"input\" when you invoke the chain:\n",
    "\n",
    " (\"user\",\"{input}\") -> This is just a tuple of two strings ‚Äî plain Python. LangChain stores this structure inside a ChatPromptTemplate.\n",
    "\n",
    " LangChain parses your message string and searches for placeholders like {input}, then fills them in with values from the dictionary you pass during .invoke(...).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88e170b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Prompt Engineering\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer. Provide me answer based on the question\"), # system prompt definition\n",
    "        (\"user\",\"{input}\") # user prompt. System prompt is used to define the behavior of the model (provided in the background). User prompt is the input to the model.\n",
    "    ]\n",
    ")\n",
    "prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9e4366",
   "metadata": {},
   "source": [
    "String PromptTemplate :\n",
    "These prompt templates are used to format a single string, and generally are used for simpler inputs. For example, a common way to construct and use a PromptTemplate is as follows:\n",
    "\n",
    "From <https://python.langchain.com/docs/concepts/prompt_templates/> \n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "\n",
    "prompt_template.invoke({\"topic\": \"cats\"})\n",
    "\n",
    "ChatPromptTemplate:\n",
    "These prompt templates are used to format a list of messages. These \"templates\" consist of a list of templates themselves. For example, a common way to construct and use a ChatPromptTemplate is as follows:\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    (\"user\", \"Tell me a joke about {topic}\")\n",
    "])\n",
    "\n",
    "prompt_template.invoke({\"topic\": \"cats\"})\n",
    "\n",
    "In the above example, this ChatPromptTemplate will construct two messages when called. The first is a system message, that has no variables to format. The second is a HumanMessage, and will be formatted by the topic variable the user passes in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d414adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001E26FBC3750>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001E26FBC3C50>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"gemma2-9b-it\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d40b1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001E26FBC3750>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001E26FBC3C50>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### chaining - the idea of langchain is to chain the components together\n",
    "chain=prompt|model\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a293571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI Engineer, I can definitely tell you about LangSmith!\n",
      "\n",
      "LangSmith is an open-source platform developed by the amazing team at Weights & Biases. It's designed to make the process of fine-tuning large language models (LLMs) much more accessible and efficient. \n",
      "\n",
      "Here are some key things to know about LangSmith:\n",
      "\n",
      "**What it does:**\n",
      "\n",
      "* **Streamlines fine-tuning:** LangSmith simplifies the often complex process of fine-tuning LLMs for specific tasks. It provides a user-friendly interface and tools that handle many of the technical details, allowing you to focus on your model's performance and goals.\n",
      "* **Collaboration & Sharing:** It fosters collaboration by enabling users to share their fine-tuned models, datasets, and training scripts with the community. This open approach accelerates progress and allows everyone to benefit from each other's work.\n",
      "* **Experiment Tracking & Management:** LangSmith integrates seamlessly with Weights & Biases, providing robust experiment tracking capabilities. You can easily monitor your training progress, compare different model architectures and hyperparameters, and reproduce your experiments with confidence.\n",
      "* **Supports Multiple LLMs:**  LangSmith isn't tied to a single LLM. It supports a variety of popular models, giving you flexibility in choosing the best tool for your task.\n",
      "\n",
      "**Why it's awesome:**\n",
      "\n",
      "* **Democratization of AI:** By making fine-tuning more accessible, LangSmith empowers individuals and smaller teams to leverage the power of LLMs without needing extensive technical expertise.\n",
      "* **Faster Innovation:** The collaborative nature of LangSmith accelerates the development of new and improved LLMs by allowing for rapid sharing and iteration of ideas.\n",
      "* **Transparency & Reproducibility:** Experiment tracking and sharing promote transparency and reproducibility in AI research, leading to more reliable and trustworthy results.\n",
      "\n",
      "**Where to learn more:**\n",
      "\n",
      "* **LangSmith Website:** [https://www.wrightsandbiases.com/blog/introducing-langsmith](https://www.wrightsandbiases.com/blog/introducing-langsmith)\n",
      "* **GitHub Repository:** [https://github.com/weights-biases/langsmith](https://github.com/weights-biases/langsmith)\n",
      "\n",
      "\n",
      "Let me know if you have any other questions about LangSmith or if you'd like to explore specific use cases!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response=chain.invoke({\"input\":\"Can you tell me something about Langsmith\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6694a12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's talk about Langsmith!\n",
      "\n",
      "Langsmith is an open-source framework designed to streamline the process of building and deploying language models (LLMs). It's a powerful tool, particularly beneficial for researchers and developers who want to experiment with and customize LLMs without getting bogged down in complex infrastructure or coding. \n",
      "\n",
      "Here's a breakdown of what makes Langsmith stand out:\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "* **Simplified LLM Development:** Langsmith provides a user-friendly interface and pre-built components that make it easier to define, train, and evaluate LLMs. This lowers the barrier to entry for newcomers to the field of LLM development.\n",
      "* **Model Card Generation:**  It automatically generates detailed model cards, which are essential for documenting and understanding the capabilities, limitations, and potential biases of your LLM. \n",
      "* **Experiment Tracking:** Langsmith helps you track different model configurations, training runs, and performance metrics. This enables you to compare results, identify improvements, and reproduce experiments easily.\n",
      "* **Open-Weight Access:**  Langsmith emphasizes transparency by making it straightforward to access and contribute to the weights (parameters) of the models it supports. This fosters collaboration and allows the community to build upon each other's work.\n",
      "\n",
      "**Benefits:**\n",
      "\n",
      "* **Faster Iteration:** The streamlined workflow in Langsmith lets you experiment with different LLM architectures, training techniques, and datasets more quickly.\n",
      "* **Reproducibility:**  The emphasis on experiment tracking and model documentation makes it easier to reproduce your results and share your work with others.\n",
      "* **Community Engagement:** The open-source nature of Langsmith encourages collaboration and knowledge sharing within the LLM community.\n",
      "\n",
      "**Who Should Use It:**\n",
      "\n",
      "* **Researchers:**  Explore new LLM architectures and training methods.\n",
      "* **Developers:** Build custom LLMs tailored to specific applications.\n",
      "* **Educators:**  Use Langsmith as a teaching tool to introduce students to LLM development.\n",
      "\n",
      "**Getting Started:**\n",
      "\n",
      "* **Documentation:** The official Langsmith documentation is a great place to start: [https://github.com/langsmithai/langsmith](https://github.com/langsmithai/langsmith)\n",
      "* **Community:**  Join the Langsmith community on Discord or GitHub for support and discussions.\n",
      "\n",
      "Let me know if you have any more questions about Langsmith or want to explore specific aspects in more detail!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### OutputParser - display the output in a specific format\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser=StrOutputParser()\n",
    "\n",
    "chain=prompt|model|output_parser\n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0221a0c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Return a JSON object.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there are multiple output parsers depending on your needs\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "output_parser=JsonOutputParser()\n",
    "output_parser.get_format_instructions() \n",
    "# 'Return a JSON object.' -> this is nothing more than a string which you could add to your system or input prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8b2ba3",
   "metadata": {},
   "source": [
    "String Output Parser mostly works for chaining purposes. You can add it directly to the chain.\n",
    "\n",
    "But for other parsers, we need to use ChatPromptTemplate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66da8aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "output_parser=JsonOutputParser()\n",
    "\n",
    "prompt=PromptTemplate( # Answer the user query - your prompt. format_instruction and query are placeholders for the input variables\n",
    "    template=\"Answer the user query \\n {format_instruction}\\n {query}\\n \", # instructions for the output parser + what context you want to add\n",
    "    input_variables=[\"query\"], # these elements (query and format_instruction) will then be fed into template\n",
    "    partial_variables={\"format_instruction\":output_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    template=\"Answer the user query \\n Return a JSON object\\n {query}\\n \", # -> this is the same as the previous prompt!\n",
    "    input_variables=[\"query\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5fe079e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='Answer the user query \\n Return a JSON object\\n {query}\\n ')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "52a5b27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Langsmith', 'description': 'Langsmith is an open-source platform for building and deploying language models. It provides a modular and customizable framework for training, evaluating, and serving language models at scale.', 'features': ['Modular architecture', 'Support for various model architectures', 'Easy integration with popular machine learning libraries', 'Scalable training and deployment', 'Comprehensive evaluation tools', 'Open-source and community-driven'], 'benefits': ['Accelerated development cycle', 'Improved model performance', 'Reduced infrastructure costs', 'Enhanced collaboration', 'Access to cutting-edge research'], 'website': 'https://www.langsmith.ai/'}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|model|output_parser\n",
    "response=chain.invoke({\"query\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f8312e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Langsmith', 'description': 'Langsmith is an open-source platform for building and deploying AI assistants.', 'key_features': ['**Modular and Extensible:** Built with composable components, allowing for customization and integration with various tools and services.', '**Fine-Tuning Capabilities:** Enables users to fine-tune pre-trained language models on their own datasets for specific tasks.', '**Data Management:** Provides tools for managing and processing conversational data, including data cleaning and annotation.', '**Deployment Flexibility:** Supports deployment on various platforms, including cloud, on-premise, and edge devices.', '**Community-Driven:**  An open-source project with a growing community of developers and contributors.', '**Focus on Privacy:** Emphasizes data privacy and security through features like on-device processing and differential privacy.'], 'use_cases': ['**Chatbots and Conversational Agents:** Building interactive chatbots for customer service, education, and entertainment.', '**Personal Assistants:** Creating personalized AI assistants that can help with scheduling, reminders, and information retrieval.', '**Content Creation:** Assisting with writing tasks such as generating articles, summarizing text, and translating languages.', '**Code Generation:** Helping developers write and debug code by providing suggestions and completing code snippets.'], 'website': 'https://www.langsmith.ai/'}\n"
     ]
    }
   ],
   "source": [
    "### Assisgnment ---Chatprompttemplate\n",
    "\n",
    "# Alternative way to do the same thing using ChatPromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\",f\"You are an expert AI Engineer. Provide me answer based on the question. {output_parser.get_format_instructions()}\"),\n",
    "    (\"user\", \"{query}\")\n",
    "])\n",
    "chain = prompt_template | model | output_parser\n",
    "response = chain.invoke({\"query\": \"Can you tell me about Langsmith?\"})\n",
    "print(response)\n",
    "\n",
    "\n",
    "### Same done by Krish Naik\n",
    "\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# prompt=ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\",\"You are an expert AI Engineer.Provide the response in json.Provide me answer based on the question\"),\n",
    "#         (\"user\",\"{input}\")\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# chain=prompt|model|output_parser\n",
    "# response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3bd1a990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LangsmithResponse': [{'description': 'Langsmith is an open-source platform designed to simplify the process of building and deploying large language models (LLMs). It aims to make LLMs more accessible to a wider range of developers and researchers by providing a user-friendly interface and a range of pre-trained models.'}, {'keyFeatures': [{'feature': [{'name': 'Model Hub'}, {'details': 'Offers a collection of pre-trained LLMs, allowing users to easily integrate powerful AI capabilities into their applications without having to train models from scratch.'}]}, {'feature': [{'name': 'Model Training'}, {'details': 'Provides tools and infrastructure for training custom LLMs, enabling users to fine-tune existing models or build entirely new ones tailored to their specific needs.'}]}, {'feature': [{'name': 'Model Deployment'}, {'details': 'Simplifies the process of deploying trained LLMs as APIs, making it easy to integrate them into applications and workflows.'}]}]}]}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "XML_output_parser=XMLOutputParser()\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\",f\"You are an expert AI Engineer. Provide me answer based on the question. {XML_output_parser.get_format_instructions()}\"),\n",
    "    (\"user\", \"{query}\")\n",
    "])\n",
    "chain = prompt_template | model | XML_output_parser\n",
    "response = chain.invoke({\"query\": \"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50822936",
   "metadata": {},
   "source": [
    "### Assigments: https://python.langchain.com/docs/how_to/#prompt-templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c1c1802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer.<response><answer>Your answer here</answer></response>.Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### OutputParser XML format\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "output_parser=XMLOutputParser()\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer.<response><answer>Your answer here</answer></response>.Provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ca6e8b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={'format_instruction': 'The output should be formatted as a XML file.\\n1. Output should conform to the tags below.\\n2. If tags are not given, make them on your own.\\n3. Remember to always open and close all the tags.\\n\\nAs an example, for the tags [\"foo\", \"bar\", \"baz\"]:\\n1. String \"<foo>\\n   <bar>\\n      <baz></baz>\\n   </bar>\\n</foo>\" is a well-formatted instance of the schema.\\n2. String \"<foo>\\n   <bar>\\n   </foo>\" is a badly-formatted instance.\\n3. String \"<foo>\\n   <tag>\\n   </tag>\\n</foo>\" is a badly-formatted instance.\\n\\nHere are the output tags:\\n```\\nNone\\n```'}, template='Answer the user query \\n {format_instruction}\\n {query}\\n ')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "output_parser=XMLOutputParser()\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    template=\"Answer the user query \\n {format_instruction}\\n {query}\\n \",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instruction\":output_parser.get_format_instructions()},\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "940f704a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"<language-model>\\n  <name>Langsmith</name>\\n  <description>Langsmith is an open-weights AI assistant built by Cohere. It's designed to be accessible and customizable, allowing developers to fine-tune it for specific tasks and domains.</description>\\n  <capabilities>\\n    <capability>Text generation</capability>\\n    <capability>Text summarization</capability>\\n    <capability>Dialogue generation</capability>\\n    <capability>Code generation</capability>\\n  </capabilities>\\n  <open-weights>True</open-weights>\\n  <developer>Cohere</developer>\\n</language-model> \\n\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 143, 'prompt_tokens': 195, 'total_tokens': 338, 'completion_time': 0.26, 'prompt_time': 0.009263582, 'queue_time': 0.161846782, 'total_time': 0.269263582}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run--245e561b-a422-40e2-adba-139230aeca1d-0' usage_metadata={'input_tokens': 195, 'output_tokens': 143, 'total_tokens': 338}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|model\n",
    "response=chain.invoke({\"query\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1eec50bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='<response><answer>LangChain is an open-source framework designed to simplify the development of applications powered by large language models (LLMs). It provides a collection of tools and components that help developers build chains of LLMs, connect them with other data sources, and manage their interactions. </answer></response> \\n' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 67, 'prompt_tokens': 39, 'total_tokens': 106, 'completion_time': 0.121818182, 'prompt_time': 0.002368796, 'queue_time': 0.159619218, 'total_time': 0.124186978}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run--7c27dbd6-22cb-4384-8c33-4f8c57a0a419-0' usage_metadata={'input_tokens': 39, 'output_tokens': 67, 'total_tokens': 106}\n"
     ]
    }
   ],
   "source": [
    "##output parser\n",
    "#from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain.output_parsers.xml import XMLOutputParser\n",
    "\n",
    "# XML Output Parser\n",
    "output_parser = XMLOutputParser()\n",
    "\n",
    "# Prompt that instructs the model to return XML\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Respond in this XML format: <response><answer>Your answer here</answer></response>\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Build the chain\n",
    "chain = prompt | model\n",
    "\n",
    "# Run the chain\n",
    "#response = chain.invoke({\"input\": \"What is LangChain?\"})\n",
    "\n",
    "raw_output =chain.invoke({\"input\": \"What is LangChain?\"})\n",
    "\n",
    "# Print result\n",
    "print(raw_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ab7431f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': \"Why couldn't the bicycle stand up by itself?\",\n",
       " 'punchline': 'Because it was two tired!'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## With Pydantic\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "model = ChatOpenAI(temperature=0.7) # by default it is gpt-3.5-turbo\n",
    "\n",
    "\n",
    "# Define your desired data structure. BaseModel is like a data class. Field is like a field in a data class.\n",
    "# You can combine output parsers with the fields you want this parser to have using pydantic package\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template -> instructions of the fields LLM should return as an output\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "36e1dcd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joke': 'Why did the scarecrow win an award? Because he was outstanding in his field!'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Without Pydantic\n",
    "joke_query = \"Tell me a joke .\"\n",
    "model = ChatOpenAI(temperature=0.7, model=\"gpt-4.1-nano-2025-04-14\")\n",
    "\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8f2ec0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Here's a shortened filmography of Tom Hanks with the movies enclosed in <movie></movie> tags:\n",
      "\n",
      "<movie>Forrest Gump</movie>\n",
      "<movie>Saving Private Ryan</movie>\n",
      "<movie>Cast Away</movie>\n",
      "<movie>Big</movie>\n",
      "<movie>Philadelphia</movie>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "actor_query = \"Generate the shortened filmography for Tom Hanks.\"\n",
    "\n",
    "output = model.invoke(\n",
    "    f\"\"\"{actor_query}\n",
    "Please enclose the movies in <movie></movie> tags\"\"\"\n",
    ")\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90caccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup='Why did the scarecrow win an award?', punchline='Because he was outstanding in his field!')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import YamlOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "model = ChatOpenAI(temperature=0.5, model=\"gpt-4.1-nano-2025-04-14\")\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke. \"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = YamlOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfed2d4",
   "metadata": {},
   "source": [
    "### Assisgment:\n",
    "Create a simple assistant that uses any LLM and should be pydantic, when we ask about any product it should give you two information product Name, product details tentative price in USD (integer). use chat Prompt Template.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
