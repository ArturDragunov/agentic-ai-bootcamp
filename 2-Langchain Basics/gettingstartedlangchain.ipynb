{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae95606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f3c76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd7e631",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getenv(\"LANGCHAIN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a95342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\") # keep these exact names as they would be looked for in langchain\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "## Langsmith Tracking And Tracing\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\" # this is for langsmith tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47124a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm=ChatOpenAI(model=\"gpt-4.1-nano-2025-04-14\") # cost optimized model for real time applications\n",
    "# llm=ChatOpenAI(model=\"gpt-4.1-mini-2025-04-14\") # for more complex tasks\n",
    "print(llm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ca146d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=llm.invoke(\"What is Agentic AI\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d6b3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8357dd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"qwen-qwq-32b\") # reasoning model - we see it chain of thought\n",
    "model.invoke(\"Hi My name is Krish\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1529904b",
   "metadata": {},
   "source": [
    " LangChain internally handles string formatting using the Jinja-like templating system. You’re not building the string yourself — you're passing a template object with variable placeholders. LangChain will later inject the value of \"input\" when you invoke the chain:\n",
    "\n",
    " (\"user\",\"{input}\") -> This is just a tuple of two strings — plain Python. LangChain stores this structure inside a ChatPromptTemplate.\n",
    "\n",
    " LangChain parses your message string and searches for placeholders like {input}, then fills them in with values from the dictionary you pass during .invoke(...).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e170b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prompt Engineering\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer. Provide me answer based on the question\"), # system prompt definition\n",
    "        (\"user\",\"{input}\") # user prompt. System prompt is used to define the behavior of the model (provided in the background). User prompt is the input to the model.\n",
    "    ]\n",
    ")\n",
    "prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9e4366",
   "metadata": {},
   "source": [
    "String PromptTemplate :\n",
    "These prompt templates are used to format a single string, and generally are used for simpler inputs. For example, a common way to construct and use a PromptTemplate is as follows:\n",
    "\n",
    "From <https://python.langchain.com/docs/concepts/prompt_templates/> \n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "\n",
    "prompt_template.invoke({\"topic\": \"cats\"})\n",
    "\n",
    "ChatPromptTemplate:\n",
    "These prompt templates are used to format a list of messages. These \"templates\" consist of a list of templates themselves. For example, a common way to construct and use a ChatPromptTemplate is as follows:\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    (\"user\", \"Tell me a joke about {topic}\")\n",
    "])\n",
    "\n",
    "prompt_template.invoke({\"topic\": \"cats\"})\n",
    "\n",
    "In the above example, this ChatPromptTemplate will construct two messages when called. The first is a system message, that has no variables to format. The second is a HumanMessage, and will be formatted by the topic variable the user passes in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d414adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"gemma2-9b-it\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d40b1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### chaining - the idea of langchain is to chain the components together\n",
    "chain=prompt|model\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a293571",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=chain.invoke({\"input\":\"Can you tell me something about Langsmith\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6694a12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### OutputParser - display the output in a specific format\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser=StrOutputParser()\n",
    "\n",
    "chain=prompt|model|output_parser\n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0221a0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are multiple output parsers depending on your needs\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "output_parser=JsonOutputParser()\n",
    "output_parser.get_format_instructions() \n",
    "# 'Return a JSON object.' -> this is nothing more than a string which you could add to your system or input prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8b2ba3",
   "metadata": {},
   "source": [
    "String Output Parser mostly works for chaining purposes. You can add it directly to the chain.\n",
    "\n",
    "But for other parsers, we need to use ChatPromptTemplate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66da8aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "output_parser=JsonOutputParser()\n",
    "\n",
    "prompt=PromptTemplate( # Answer the user query - your prompt. format_instruction and query are placeholders for the input variables\n",
    "    template=\"Answer the user query \\n {format_instruction}\\n {query}\\n \", # instructions for the output parser + what context you want to add\n",
    "    input_variables=[\"query\"], # these elements (query and format_instruction) will then be fed into template\n",
    "    partial_variables={\"format_instruction\":output_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    template=\"Answer the user query \\n Return a JSON object\\n {query}\\n \", # -> this is the same as the previous prompt!\n",
    "    input_variables=[\"query\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe079e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a5b27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=prompt|model|output_parser\n",
    "response=chain.invoke({\"query\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8312e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Assisgnment ---Chatprompttemplate\n",
    "\n",
    "# Alternative way to do the same thing using ChatPromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\",f\"You are an expert AI Engineer. Provide me answer based on the question. {output_parser.get_format_instructions()}\"),\n",
    "    (\"user\", \"{query}\")\n",
    "])\n",
    "chain = prompt_template | model | output_parser\n",
    "response = chain.invoke({\"query\": \"Can you tell me about Langsmith?\"})\n",
    "print(response)\n",
    "\n",
    "\n",
    "### Same done by Krish Naik\n",
    "\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# prompt=ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\",\"You are an expert AI Engineer.Provide the response in json.Provide me answer based on the question\"),\n",
    "#         (\"user\",\"{input}\")\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# chain=prompt|model|output_parser\n",
    "# response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd1a990",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "XML_output_parser=XMLOutputParser()\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\",f\"You are an expert AI Engineer. Provide me answer based on the question. {XML_output_parser.get_format_instructions()}\"),\n",
    "    (\"user\", \"{query}\")\n",
    "])\n",
    "chain = prompt_template | model | XML_output_parser\n",
    "response = chain.invoke({\"query\": \"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50822936",
   "metadata": {},
   "source": [
    "### Assigments: https://python.langchain.com/docs/how_to/#prompt-templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1c1802",
   "metadata": {},
   "outputs": [],
   "source": [
    "### OutputParser XML format\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "output_parser=XMLOutputParser()\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer.<response><answer>Your answer here</answer></response>.Provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca6e8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "output_parser=XMLOutputParser()\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    template=\"Answer the user query \\n {format_instruction}\\n {query}\\n \",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instruction\":output_parser.get_format_instructions()},\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940f704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=prompt|model\n",
    "response=chain.invoke({\"query\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eec50bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##output parser\n",
    "#from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain.output_parsers.xml import XMLOutputParser\n",
    "\n",
    "# XML Output Parser\n",
    "output_parser = XMLOutputParser()\n",
    "\n",
    "# Prompt that instructs the model to return XML\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Respond in this XML format: <response><answer>Your answer here</answer></response>\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Build the chain\n",
    "chain = prompt | model\n",
    "\n",
    "# Run the chain\n",
    "#response = chain.invoke({\"input\": \"What is LangChain?\"})\n",
    "\n",
    "raw_output =chain.invoke({\"input\": \"What is LangChain?\"})\n",
    "\n",
    "# Print result\n",
    "print(raw_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7431f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## With Pydantic\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "model = ChatOpenAI(temperature=0.7) # by default it is gpt-3.5-turbo\n",
    "\n",
    "\n",
    "# Define your desired data structure. BaseModel is like a data class. Field is like a field in a data class.\n",
    "# You can combine output parsers with the fields you want this parser to have using pydantic package\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template -> instructions of the fields LLM should return as an output\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e1dcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Without Pydantic\n",
    "joke_query = \"Tell me a joke .\"\n",
    "model = ChatOpenAI(temperature=0.7, model=\"gpt-4.1-nano-2025-04-14\")\n",
    "\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2ec0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actor_query = \"Generate the shortened filmography for Tom Hanks.\"\n",
    "\n",
    "output = model.invoke(\n",
    "    f\"\"\"{actor_query}\n",
    "Please enclose the movies in <movie></movie> tags\"\"\"\n",
    ")\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90caccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import YamlOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "model = ChatOpenAI(temperature=0.5, model=\"gpt-4.1-nano-2025-04-14\")\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke. \"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = YamlOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfed2d4",
   "metadata": {},
   "source": [
    "### Assisgment:\n",
    "Create a simple assistant that uses any LLM and should be pydantic, when we ask about any product it should give you two information product Name, product details tentative price in USD (integer). use chat Prompt Template.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
